{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hello World vLLM\n",
        "\n",
        "## TL;DR\n",
        "The [notebook](./Hello_World_vLLM.ipynb) is a concise, quick-start guide that sets up vLLM, loads a 125M model, demonstrates the generation of replies, and showcases the manipulation of the model's log probabilities and tokens. It is a minimal template for experiments with larger models, streaming, and benchmarking.\n",
        "\n",
        "## High-level flow:\n",
        "1. Environment bootstrap\n",
        "2. LLM instantiation: LLM(model= \"facebook/opt-125m\") creates a small model that fits on almost any GPU.\n",
        "3. Prompt-to-Text demo: builds a list of prompts, sets sampling parameters, and generates a set of replies for each prompt, in three different use cases.\n",
        "4. Find the top three generated texts based on their tokens' average log probability.\n",
        "\n",
        "## Prompt-to-Text use cases:\n",
        "This notebook shows three basic use cases of generating text replies from prompts:\n",
        "Single prompt to single reply\n",
        "Single prompt to many replies\n",
        "Many prompts to a single reply (each of the prompts)\n",
        "\n",
        "## Top three replies:\n",
        "The last cell of this notebook contains a program that generates 10 different replies for a single prompt. Then, it calculates the average log probability for each reply based on the probabilities of its selected tokens.<br>\n",
        "It sorts the outputs according to the calculation result and displays the top three.\n",
        "\n",
        "## Possible simple extensions for this notebook:\n",
        "1. Load a larger model, such as meta-llama/Llama-3-8b if hardware allows.\n",
        "2. Wrap 'llm.generate' in a FastAPI endpoint and load-test it.\n",
        "3. Compare performance with Hugging Face TextGenerationInference.\n",
        "\n",
        "Key references\n",
        "Official docs at https://vllm.ai/"
      ],
      "metadata": {
        "id": "7uLhdLPh-QdQ"
      },
      "id": "7uLhdLPh-QdQ"
    },
    {
      "cell_type": "markdown",
      "id": "2243c16f-c7fc-412a-ba71-68960695bce6",
      "metadata": {
        "id": "2243c16f-c7fc-412a-ba71-68960695bce6"
      },
      "source": [
        "# Initializing environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f51966-8fb4-41d1-92e2-6c7f99913456",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T13:58:22.341091Z",
          "iopub.status.busy": "2025-07-22T13:58:22.340598Z",
          "iopub.status.idle": "2025-07-22T13:58:22.349277Z",
          "shell.execute_reply": "2025-07-22T13:58:22.348934Z",
          "shell.execute_reply.started": "2025-07-22T13:58:22.341082Z"
        },
        "tags": [],
        "id": "39f51966-8fb4-41d1-92e2-6c7f99913456",
        "outputId": "dda08d03-15cd-45c9-bd19-1fbd611a155e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "os.getcwd()='/notebooks'\n",
            "sys.executable='/notebooks/vllm_venv/bin/python'\n",
            "==================================================\n",
            "sys.path[0] = /notebooks/vllm\n",
            "sys.path[1] = /usr/lib/python311.zip\n",
            "sys.path[2] = /usr/lib/python3.11\n",
            "sys.path[3] = /usr/lib/python3.11/lib-dynload\n",
            "sys.path[4] = \n",
            "sys.path[5] = /notebooks/vllm_venv/lib/python3.11/site-packages\n",
            "sys.path[6] = __editable__.vllm-0.9.2+cu118.finder.__path_hook__\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "print(f'{os.getcwd()=}')\n",
        "print(f'{sys.executable=}')\n",
        "\n",
        "sys_path_vllm = '/notebooks/vllm'\n",
        "if os.path.isdir(sys_path_vllm) and sys_path_vllm not in sys.path:\n",
        "    sys.path.insert(0, sys_path_vllm)\n",
        "\n",
        "print('='*50)\n",
        "for i,p in enumerate(sys.path):\n",
        "    print(f'sys.path[{i}] = {p}')\n",
        "print('='*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64991368-5402-4ce9-8474-af761df49b5a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T13:58:22.350105Z",
          "iopub.status.busy": "2025-07-22T13:58:22.349769Z",
          "iopub.status.idle": "2025-07-22T13:59:02.402917Z",
          "shell.execute_reply": "2025-07-22T13:59:02.402454Z",
          "shell.execute_reply.started": "2025-07-22T13:58:22.350095Z"
        },
        "tags": [],
        "id": "64991368-5402-4ce9-8474-af761df49b5a",
        "outputId": "9a64a339-e967-443e-f3e0-80e0c67a35b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 13:58:56 [__init__.py:244] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from vllm import LLM, SamplingParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459a83f0-7b7f-4e80-aa6a-c54f9605774e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T13:59:02.405832Z",
          "iopub.status.busy": "2025-07-22T13:59:02.405331Z",
          "iopub.status.idle": "2025-07-22T14:00:04.982550Z",
          "shell.execute_reply": "2025-07-22T14:00:04.980631Z",
          "shell.execute_reply.started": "2025-07-22T13:59:02.405815Z"
        },
        "colab": {
          "referenced_widgets": [
            "d7217bd103344e03be8259294cf19501",
            "2f3c47962623413a9e0554830aecb710",
            "6d1eee0a60b747d8a3c9228fc091bc1f",
            "a789196c1dea4e828fa3bc7d324be7b0",
            "55e01c34ce2f4121be44de325a19bbe0",
            "fffacb99679642ef9ad45bb6bc7a6131",
            "e8323b1251784119a1a9ea5b2c0b102a",
            "b34ff753dc7344f18ed667b29aaf6b3f"
          ]
        },
        "id": "459a83f0-7b7f-4e80-aa6a-c54f9605774e",
        "outputId": "e9ea36ea-80e8-4f8f-cf83-4b2458b85732"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7217bd103344e03be8259294cf19501",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 13:59:12 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
            "INFO 07-22 13:59:12 [config.py:1472] Using max model len 2048\n",
            "INFO 07-22 13:59:14 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f3c47962623413a9e0554830aecb710",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d1eee0a60b747d8a3c9228fc091bc1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a789196c1dea4e828fa3bc7d324be7b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55e01c34ce2f4121be44de325a19bbe0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fffacb99679642ef9ad45bb6bc7a6131",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 13:59:14 [core.py:526] Waiting for init message from front-end.\n",
            "INFO 07-22 13:59:14 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "INFO 07-22 13:59:18 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-22 13:59:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-22 13:59:18 [gpu_model_runner.py:1770] Starting to load model facebook/opt-125m...\n",
            "INFO 07-22 13:59:18 [gpu_model_runner.py:1775] Loading model from scratch...\n",
            "INFO 07-22 13:59:18 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-22 13:59:18 [weight_utils.py:292] Using model weights format ['*.bin']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8323b1251784119a1a9ea5b2c0b102a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 13:59:20 [weight_utils.py:308] Time spent downloading weights for facebook/opt-125m: 1.244602 seconds\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b34ff753dc7344f18ed667b29aaf6b3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 13:59:20 [default_loader.py:272] Loading weights took 0.22 seconds\n",
            "INFO 07-22 13:59:20 [gpu_model_runner.py:1801] Model loading took 0.2389 GiB and 1.732135 seconds\n",
            "INFO 07-22 13:59:29 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/7b4b1a726a/rank_0_0/backbone for vLLM's torch.compile\n",
            "INFO 07-22 13:59:29 [backends.py:519] Dynamo bytecode transform time: 8.33 s\n",
            "INFO 07-22 13:59:31 [backends.py:181] Cache the graph of shape None for later use\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[rank0]:W0722 13:59:31.641000 129 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 13:59:36 [backends.py:193] Compiling a graph for general shape takes 6.93 s\n",
            "INFO 07-22 13:59:38 [monitor.py:34] torch.compile takes 15.26 s in total\n",
            "INFO 07-22 13:59:41 [gpu_worker.py:232] Available KV cache memory: 20.54 GiB\n",
            "INFO 07-22 13:59:41 [kv_cache_utils.py:716] GPU KV cache size: 598,240 tokens\n",
            "INFO 07-22 13:59:41 [kv_cache_utils.py:720] Maximum concurrency for 2,048 tokens per request: 292.11x\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:22<00:00,  2.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-22 14:00:04 [gpu_model_runner.py:2326] Graph capturing finished in 23 secs, took 0.21 GiB\n",
            "INFO 07-22 14:00:04 [core.py:172] init engine (profile, create kv cache, warmup model) took 44.00 seconds\n"
          ]
        }
      ],
      "source": [
        "llm = LLM(model=\"facebook/opt-125m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56452426-e723-4f58-b897-5c84631e9c60",
      "metadata": {
        "tags": [],
        "id": "56452426-e723-4f58-b897-5c84631e9c60"
      },
      "source": [
        "# Single prompt --> Single output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d6e94c-9217-4b31-8bb7-a8f609954a4c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:00:04.983612Z",
          "iopub.status.busy": "2025-07-22T14:00:04.983406Z",
          "iopub.status.idle": "2025-07-22T14:00:05.338247Z",
          "shell.execute_reply": "2025-07-22T14:00:05.337849Z",
          "shell.execute_reply.started": "2025-07-22T14:00:04.983597Z"
        },
        "colab": {
          "referenced_widgets": [
            "21d259091dfe4b9f86cdcd7430805d78",
            "538024c4861c45e3897b6576a2b9e4dc"
          ]
        },
        "id": "74d6e94c-9217-4b31-8bb7-a8f609954a4c",
        "outputId": "2b9d0328-62f6-4aba-e628-3afad1e5bf31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21d259091dfe4b9f86cdcd7430805d78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "538024c4861c45e3897b6576a2b9e4dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Hello, my name is\n",
            "Generated:  Joel, I'm a software developer and I'm trying to make a mobile app. The problem is\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello, my name is\"\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=20)\n",
        "\n",
        "outputs = llm.generate(prompt, sampling_params)\n",
        "\n",
        "for output in outputs:\n",
        "    print(\"Prompt:\", output.prompt)\n",
        "    print(\"Generated:\", output.outputs[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "819cd64b-3c8d-4ad9-99d1-8e5b20be19ca",
      "metadata": {
        "id": "819cd64b-3c8d-4ad9-99d1-8e5b20be19ca"
      },
      "source": [
        "# Single prompt --> Many outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0fb81b3-76c4-4076-9386-48155d47630d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:00:05.339686Z",
          "iopub.status.busy": "2025-07-22T14:00:05.339315Z",
          "iopub.status.idle": "2025-07-22T14:00:05.386034Z",
          "shell.execute_reply": "2025-07-22T14:00:05.385609Z",
          "shell.execute_reply.started": "2025-07-22T14:00:05.339671Z"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "fcb9f1507320459b981eb1d794be2339",
            "c725d7543c144cedaae482e3d0831d32"
          ]
        },
        "id": "f0fb81b3-76c4-4076-9386-48155d47630d",
        "outputId": "ba4ba810-a31c-4450-d10c-89d74e498a86"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcb9f1507320459b981eb1d794be2339",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c725d7543c144cedaae482e3d0831d32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Hello, my name is\n",
            "Generated #1:  Yuvraj and I’m 25 years old. I am a happy, conservative Indian\n",
            "Generated #2:  James.\n",
            "I'm a Psychologist.\n",
            "You're better off being a broke man.\n",
            "\n",
            "Generated #3:  Liz, I'm going to be a waitress at a coffee shop.\n",
            "Liz, please don\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello, my name is\"\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=20, n=3)\n",
        "\n",
        "# Generate multiple completions\n",
        "outputs = llm.generate(prompt, sampling_params)\n",
        "\n",
        "# Print all completions\n",
        "for output in outputs:\n",
        "    print(\"Prompt:\", output.prompt)\n",
        "    for i, result in enumerate(output.outputs):\n",
        "        print(f\"Generated #{i+1}:\", result.text)\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb42137-347f-4ab4-abe6-e076494ba0ed",
      "metadata": {
        "id": "1cb42137-347f-4ab4-abe6-e076494ba0ed"
      },
      "source": [
        "# Many prompts --> Single output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f65b3d-7630-4c63-a49a-0c09519c77cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:00:05.387484Z",
          "iopub.status.busy": "2025-07-22T14:00:05.387352Z",
          "iopub.status.idle": "2025-07-22T14:00:05.457779Z",
          "shell.execute_reply": "2025-07-22T14:00:05.457318Z",
          "shell.execute_reply.started": "2025-07-22T14:00:05.387470Z"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "980b6c0038ec4dd2987a6861aab33371",
            "7dfdb25bbfd64b88a5ad04fa526b13d2"
          ]
        },
        "id": "72f65b3d-7630-4c63-a49a-0c09519c77cd",
        "outputId": "5a5a5be1-e8ad-46d1-8959-5f618fc96098"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "980b6c0038ec4dd2987a6861aab33371",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dfdb25bbfd64b88a5ad04fa526b13d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: What is the capital of France?\n",
            "Response: \n",
            "France is a capital of France, because of its size and population. It’s the capital of Germany and it’s the capital\n",
            "--------------------------------------------------\n",
            "Prompt: Write a short poem about the ocean.\n",
            "Response: \n",
            "\n",
            "I am going to leave this link here for now, so I can stop posting. What I will do is try to write a poem about\n",
            "--------------------------------------------------\n",
            "Prompt: Explain what a black hole is.\n",
            "Response: \n",
            "It seems to be a black hole with a red hole. I can't see where it is, but I've seen it happen.\n",
            "--------------------------------------------------\n",
            "Prompt: Give me a Python one-liner to reverse a string.\n",
            "Response: \n",
            "\n",
            "John\n",
            "\n",
            "#3: \"But the world is full of contradictions, and what a contradiction!\"\n",
            "#4: \"If it's\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Define a batch of prompts\n",
        "prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Write a short poem about the ocean.\",\n",
        "    \"Explain what a black hole is.\",\n",
        "    \"Give me a Python one-liner to reverse a string.\"\n",
        "]\n",
        "\n",
        "# Set generation configuration\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=30)\n",
        "\n",
        "# Run batched generation\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# Print each prompt with its response\n",
        "for output in outputs:\n",
        "    print(\"Prompt:\", output.prompt)\n",
        "    print(\"Response:\", output.outputs[0].text)\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d310835-84d1-44a6-8c6b-5da2e70e78b7",
      "metadata": {
        "id": "8d310835-84d1-44a6-8c6b-5da2e70e78b7"
      },
      "source": [
        "# Top 3 completions for a single prompt based on likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9869bc-5da6-4527-aaeb-8c30ca544e49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-22T14:00:05.459392Z",
          "iopub.status.busy": "2025-07-22T14:00:05.459247Z",
          "iopub.status.idle": "2025-07-22T14:00:05.631190Z",
          "shell.execute_reply": "2025-07-22T14:00:05.630676Z",
          "shell.execute_reply.started": "2025-07-22T14:00:05.459378Z"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "ea11a5cd0a3d4119b27d53c28a781081",
            "95bf6254d0514221b294b773b1734653"
          ]
        },
        "id": "8a9869bc-5da6-4527-aaeb-8c30ca544e49",
        "outputId": "4dc44068-57a0-47f6-a006-27a098b16b58"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea11a5cd0a3d4119b27d53c28a781081",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95bf6254d0514221b294b773b1734653",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt=Hello, my name is --> [1]: completion=' Melissa and I am a thirty-six year old English teacher. I love teaching and I love spending'  avg_logprob=-2.1378440018743277\n",
            "prompt=Hello, my name is --> [2]: completion=' Mika, I used to be a graphic artist, I would love to find more of your work'  avg_logprob=-2.429113742336631\n",
            "prompt=Hello, my name is --> [3]: completion=' Minnie and i am a 6\\'2\" country singer. I\\'m a singer in the St'  avg_logprob=-2.8559442430734636\n",
            "prompt=Hello, my name is --> [4]: completion=' Ramona and I am a 15 year old girl. My boyfriend is a guy and we have been'  avg_logprob=-2.052470563072711\n",
            "prompt=Hello, my name is --> [5]: completion=' Tom, and I\\'m the owner of the award-winning and award-winning book \"The Book'  avg_logprob=-2.0749319683993237\n",
            "prompt=Hello, my name is --> [6]: completion=\" Subramaniam.\\nHello, it's Subramaniam.\"  avg_logprob=-2.02913129364606\n",
            "prompt=Hello, my name is --> [7]: completion=\" Jack and I'm a newbie to Reddit and I'm looking for a good place to post my\"  avg_logprob=-1.8606620140373706\n",
            "prompt=Hello, my name is --> [8]: completion=\" Nicky. I am a 30-something student, but I have been living in my parents'\"  avg_logprob=-2.2518357947468757\n",
            "prompt=Hello, my name is --> [9]: completion=' Ivette. I am a Spanish speaker from Madrid. I have been studying English for a long time'  avg_logprob=-2.2196583792567255\n",
            "prompt=Hello, my name is --> [10]: completion=' Jaxx, I’m a 20 year old male from the US. I am really'  avg_logprob=-1.9841136791976168\n",
            "\n",
            "Top 3 completions by average log-likelihood:\n",
            "\n",
            "#1 avg_logprob=-1.8606620140373706 :\n",
            " Jack and I'm a newbie to Reddit and I'm looking for a good place to post my\n",
            "\n",
            "#2 avg_logprob=-1.9841136791976168 :\n",
            " Jaxx, I’m a 20 year old male from the US. I am really\n",
            "\n",
            "#3 avg_logprob=-2.02913129364606 :\n",
            " Subramaniam.\n",
            "Hello, it's Subramaniam.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello, my name is\"\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.7,\n",
        "    max_tokens=20,\n",
        "    n=10,\n",
        "    logprobs=1  # Request logprobs for each token\n",
        ")\n",
        "\n",
        "outputs = llm.generate(prompt, sampling_params)\n",
        "\n",
        "# Collect completions\n",
        "for output in outputs:\n",
        "    pr = output.prompt\n",
        "    print_results = len(output.outputs) <= 10\n",
        "    scored_completions = []\n",
        "    for ind_res,res in enumerate(output.outputs):\n",
        "        logprobs = []\n",
        "        for token_id, token_logprobs_dict in zip(res.token_ids, res.logprobs):\n",
        "            logprob_entry = token_logprobs_dict.get(token_id, None)\n",
        "            if logprob_entry is not None:\n",
        "                logprobs.append(logprob_entry.logprob)\n",
        "\n",
        "        avg_logprob = sum(logprobs) / max(len(logprobs),1)\n",
        "        completion = res.text\n",
        "        scored_completions.append((avg_logprob,completion))\n",
        "        if print_results:\n",
        "            print(f'prompt={pr} --> [{ind_res+1}]: {completion=}  {avg_logprob=}')\n",
        "\n",
        "# Sort by highest average log-probability\n",
        "scored_completions.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# Print top 3 completions\n",
        "print(\"\\nTop 3 completions by average log-likelihood:\")\n",
        "for i, (avg_logprob, text) in enumerate(scored_completions[:3]):\n",
        "    print(f\"\\n#{i+1} {avg_logprob=} :\\n{text}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (vllm_venv)",
      "language": "python",
      "name": "vllm_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}